{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92875a06",
   "metadata": {},
   "source": [
    "# Preparação do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344a98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import torch_directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9d760ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(rewards, window_size=10):\n",
    "    n = len(rewards)\n",
    "    grouped_means = [np.mean(rewards[i:i+window_size]) for i in range(0, n, window_size)]\n",
    "    grouped_indices = [i for i in range(0, n, window_size)]\n",
    "    return grouped_indices, grouped_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20bfe5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(train_rewards, test_rewards=None, title=None, max_reward=None, window_size=10):\n",
    "    if test_rewards is not None:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "        axes = [axes]\n",
    "\n",
    "    # TRAINING\n",
    "    axes[0].plot(train_rewards, label='Reward (puro)', color='skyblue', alpha=0.6)\n",
    "    train_x, train_avg = moving_average(train_rewards, window_size)\n",
    "    axes[0].plot(train_x, train_avg, label=f'Média a cada {window_size}', color='orange')\n",
    "    axes[0].set_title(\"Training\")\n",
    "    axes[0].set_xlabel(\"Episodes\")\n",
    "    axes[0].set_ylabel(\"Reward\")\n",
    "\n",
    "    if max_reward is not None:\n",
    "        axes[0].axhline(max_reward, color='red', linestyle='--', label='Max Reward')\n",
    "\n",
    "    axes[0].legend()\n",
    "\n",
    "    # TESTING\n",
    "    if test_rewards is not None:\n",
    "        axes[1].plot(test_rewards, label='Reward (puro)', color='skyblue', alpha=0.6)\n",
    "        test_x, test_avg = moving_average(test_rewards, window_size)\n",
    "        axes[1].plot(test_x, test_avg, label=f'Média a cada {window_size}', color='orange')\n",
    "        axes[1].set_title(\"Testing\")\n",
    "        axes[1].set_xlabel(\"Episodes\")\n",
    "        axes[1].set_ylabel(\"Reward\")\n",
    "\n",
    "        if max_reward is not None:\n",
    "            axes[1].axhline(max_reward, color='red', linestyle='--', label='Max Reward')\n",
    "\n",
    "        axes[1].legend()\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20529315",
   "metadata": {},
   "source": [
    "# Prioritized Experience Replay (PER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac7aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "   def __init__(self,\n",
    "                capacity: int,\n",
    "                observation_dim: int,\n",
    "                alpha: float = 0.6):\n",
    "      assert alpha >= 0\n",
    "      self._alpha = alpha\n",
    "      \n",
    "      self._capacity = capacity\n",
    "      self._size = 0\n",
    "      self._index = 0\n",
    "      \n",
    "      self._memory = {\n",
    "         \"states\": np.zeros((capacity, observation_dim), dtype=np.float32),\n",
    "         \"actions\": np.zeros((capacity,), dtype=np.int32),\n",
    "         \"rewards\": np.zeros((capacity,), dtype=np.float32),\n",
    "         \"next_states\": np.zeros((capacity, observation_dim), dtype=np.float32),\n",
    "         \"dones\": np.zeros((capacity,), dtype=np.int32),\n",
    "         \"priorities\": np.zeros((capacity,), dtype=np.float32)\n",
    "      }\n",
    "      self._max_priority = 1.0\n",
    "   \n",
    "   def __len__(self):\n",
    "      return self._size\n",
    "\n",
    "   def _increment_index(self):\n",
    "      self._index = (self._index + 1) % self._capacity\n",
    "      self._size = min(self._size + 1, self._capacity)\n",
    "   \n",
    "   def store(self,\n",
    "             state: np.ndarray,\n",
    "             action: int,\n",
    "             reward: float,\n",
    "             next_state: np.ndarray,\n",
    "             done: bool):\n",
    "      self._memory['states'][self._index] = state\n",
    "      self._memory['actions'][self._index] = action\n",
    "      self._memory['rewards'][self._index] = reward\n",
    "      self._memory['next_states'][self._index] = next_state\n",
    "      self._memory['dones'][self._index] = np.int32(done)\n",
    "      self._memory['priorities'][self._index] = self._max_priority ** self._alpha\n",
    "      \n",
    "      self._increment_index()\n",
    "   \n",
    "   def update_priorities(self, indices: list[int], errors: np.ndarray):\n",
    "      assert len(indices) == len(errors)\n",
    "      \n",
    "      for idx, error in zip(indices, errors):\n",
    "         priority = float((abs(error) + 1e-6) ** self._alpha)\n",
    "         self._memory['priorities'][idx] = priority\n",
    "         \n",
    "         self._max_priority = max(self._max_priority, priority)\n",
    "   \n",
    "   def sample(self, batch_size: int, beta: float = 0.4):\n",
    "      assert len(self) >= batch_size\n",
    "      \n",
    "      priorities = self._memory['priorities'][:self._size]\n",
    "      if priorities.sum() == 0:\n",
    "         priorities += 1e-6\n",
    "      \n",
    "      probs = priorities\n",
    "      probs /= probs.sum()\n",
    "      \n",
    "      indices = np.random.choice(self._size, batch_size, p=probs)\n",
    "      weights = (self._size * probs[indices]) ** (-beta)\n",
    "      weights = weights / weights.max()\n",
    "      \n",
    "      return (\n",
    "         self._memory['states'][indices],\n",
    "         self._memory['actions'][indices],\n",
    "         self._memory['rewards'][indices],\n",
    "         self._memory['next_states'][indices],\n",
    "         self._memory['dones'][indices],\n",
    "         weights.astype(np.float32),\n",
    "         indices\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4ceb8",
   "metadata": {},
   "source": [
    "# Definição da Rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a53e4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "   def __init__(self, observation_shape, action_shape):\n",
    "      super(DQN, self).__init__()\n",
    "      self.fc1 = nn.Linear(observation_shape[0], 128)\n",
    "      self.fc2 = nn.Linear(128, 128)\n",
    "      self.fc3 = nn.Linear(128, action_shape)\n",
    "   \n",
    "   def forward(self, x):\n",
    "      x = torch.relu(self.fc1(x))\n",
    "      x = torch.relu(self.fc2(x))\n",
    "      x = self.fc3(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57048310",
   "metadata": {},
   "source": [
    "# Definição do Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a1538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedDQNAgent:\n",
    "   def __init__(self, env,\n",
    "                epsilon=1,\n",
    "                alpha=1e-3,\n",
    "                gamma=0.999,\n",
    "                epsilon_decay=0.998,\n",
    "                epsilon_min=0.01,\n",
    "                batch_size=64,\n",
    "                replay_buffer_size=100000,\n",
    "                beta=0.4,\n",
    "                beta_decay=1e-5):\n",
    "      self.epsilon = epsilon\n",
    "      self.alpha = alpha\n",
    "      self.gamma = gamma\n",
    "      self.epsilon_decay = epsilon_decay\n",
    "      self.epsilon_min = epsilon_min\n",
    "      self.batch_size = batch_size\n",
    "      self.beta = beta\n",
    "      self.beta_decay = beta_decay\n",
    "      \n",
    "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "      \n",
    "      print(self.device)\n",
    "         \n",
    "      self.env = env\n",
    "      \n",
    "      self.dqn = DQN(env.observation_space.shape, env.action_space.n).to(self.device)\n",
    "      self.dqn_target = DQN(env.observation_space.shape, env.action_space.n).to(self.device)\n",
    "\n",
    "      self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "      self.dqn_target.eval()\n",
    "      \n",
    "      self.replay = PrioritizedReplayBuffer(replay_buffer_size, env.observation_space.shape[0])\n",
    "      \n",
    "      self.optimizer = optim.Adam(self.dqn.parameters(), lr=alpha)\n",
    "      self.loss = nn.SmoothL1Loss(reduction='none')\n",
    "   \n",
    "   def _update_dqn_target(self):\n",
    "      self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "   def act(self, state):\n",
    "      self.dqn.eval()\n",
    "      \n",
    "      if np.random.rand() < self.epsilon:\n",
    "         return self.env.action_space.sample()\n",
    "      \n",
    "      with torch.no_grad():\n",
    "         state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "         action = self.dqn(state).argmax(dim=-1).item()\n",
    "      \n",
    "      return action\n",
    "   \n",
    "   def update(self):\n",
    "      self.dqn.train()\n",
    "      \n",
    "      states, actions, rewards, next_states, dones, weights, indices = self.replay.sample(self.batch_size, self.beta)\n",
    "      \n",
    "      states = torch.as_tensor(states).to(self.device)\n",
    "      actions = torch.as_tensor(actions).to(self.device).unsqueeze(-1)\n",
    "      rewards = torch.as_tensor(rewards).to(self.device).unsqueeze(-1)\n",
    "      next_states = torch.as_tensor(next_states).to(self.device)\n",
    "      dones = torch.as_tensor(dones).to(self.device).unsqueeze(-1)\n",
    "      weights = torch.as_tensor(weights).to(self.device).unsqueeze(-1)\n",
    "      \n",
    "      q_eval = self.dqn(states).gather(-1, actions.long())\n",
    "      \n",
    "      with torch.no_grad():\n",
    "         next_actions = self.dqn(next_states).argmax(dim=1, keepdim=True)\n",
    "         q_next = self.dqn_target(next_states).gather(1, next_actions)\n",
    "         q_target = rewards + self.gamma * q_next * (1 - dones)\n",
    "      \n",
    "      losses = self.loss(q_eval, q_target)\n",
    "      loss = (losses * weights).mean()\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "      \n",
    "      td_errors = (q_eval - q_target).detach().cpu().numpy().squeeze()\n",
    "      self.replay.update_priorities(indices, td_errors)\n",
    "      \n",
    "      self.beta *= 1 + self.beta_decay if self.beta < 1 else 1\n",
    "         \n",
    "   def train(self, episodes, verbose=False):\n",
    "      progress = tqdm(range(1, episodes + 1), desc=\"Training\", disable=not verbose)\n",
    "      rewards = []\n",
    "            \n",
    "      for episode in progress:\n",
    "         state, _ = self.env.reset()\n",
    "         episode_reward = 0\n",
    "         done = False\n",
    "         \n",
    "         while not done:\n",
    "            action = self.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            self.replay.store(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if len(self.replay) > self.batch_size:\n",
    "               self.update()\n",
    "         \n",
    "         if episode % 50 == 0:\n",
    "            self._update_dqn_target()\n",
    "         \n",
    "         self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "         \n",
    "         rewards.append(episode_reward)\n",
    "         progress.set_postfix({ \"Reward\": episode_reward, \"Epsilon\": self.epsilon })\n",
    "         \n",
    "      return rewards\n",
    "   \n",
    "   def evaluate(self, episodes, verbose=False):\n",
    "      progress = tqdm(range(1, episodes + 1), desc=\"Evaluating\", disable=not verbose)\n",
    "      rewards = []\n",
    "      \n",
    "      epsilon_real = self.epsilon\n",
    "      self.epsilon = 0\n",
    "      \n",
    "      self.dqn.eval()\n",
    "      \n",
    "      for episode in progress:\n",
    "         state, _ = self.env.reset()\n",
    "         episode_reward = 0\n",
    "         done = False\n",
    "         \n",
    "         while not done:\n",
    "            action = self.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "         rewards.append(episode_reward)\n",
    "         progress.set_postfix({ \"Reward\": episode_reward })\n",
    "      \n",
    "      self.epsilon = epsilon_real\n",
    "         \n",
    "      return rewards\n",
    "\n",
    "   def save(self, path):\n",
    "      torch.save(self.dqn.state_dict(), path)\n",
    "   \n",
    "   def load(self, path):\n",
    "      self.dqn.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b873c4",
   "metadata": {},
   "source": [
    "# Ambientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c253419",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [\n",
    "    (\"CartPole-v1\",       \"CartPole\",       \"cartpole_per.pt\",      500),\n",
    "    (\"Acrobot-v1\",        \"Acrobot\",        \"acrobot_per.pt\",       0),\n",
    "    (\"MountainCar-v0\",    \"MountainCar\",    \"mountaincar_per.pt\",   -50),\n",
    "    (\"LunarLander-v3\",    \"LunarLander\",    \"lunarlander_per.pt\",   200),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4bdff72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CartPole...\n",
      "privateuseone:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                                          | 1/1000 [00:00<02:56,  5.68it/s, Reward=33, Epsilon=0.998]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DirectML scatter doesn't allow partially modified dimensions. Please update the dimension so that the indices and input only differ in the provided dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m env = gym.make(env_name)\n\u001b[32m      5\u001b[39m agent = PrioritizedDQNAgent(env)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m train_rewards = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m test_rewards = agent.evaluate(\u001b[32m100\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m agent.save(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcheckpoint/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mPrioritizedDQNAgent.train\u001b[39m\u001b[34m(self, episodes, verbose)\u001b[39m\n\u001b[32m    102\u001b[39m    episode_reward += reward\n\u001b[32m    104\u001b[39m    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.replay) > \u001b[38;5;28mself\u001b[39m.batch_size:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m episode % \u001b[32m50\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m    108\u001b[39m    \u001b[38;5;28mself\u001b[39m._update_dqn_target()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mPrioritizedDQNAgent.update\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     76\u001b[39m loss = (losses * weights).mean()\n\u001b[32m     78\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m     82\u001b[39m td_errors = (q_eval - q_target).detach().cpu().numpy().squeeze()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.11/envs/rl-env/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.11/envs/rl-env/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.11/envs/rl-env/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: DirectML scatter doesn't allow partially modified dimensions. Please update the dimension so that the indices and input only differ in the provided dimension."
     ]
    }
   ],
   "source": [
    "for env_name, env_title, path, max_reward in envs:\n",
    "   print(f\"Training {env_title}...\")\n",
    "   \n",
    "   env = gym.make(env_name)\n",
    "   agent = PrioritizedDQNAgent(env)\n",
    "   \n",
    "   train_rewards = agent.train(1000, verbose=True)\n",
    "   test_rewards = agent.evaluate(100, verbose=True)\n",
    "   \n",
    "   agent.save(f'checkpoint/{path}')\n",
    "   \n",
    "   plot_rewards(train_rewards, test_rewards, f\"{env_title} - Prioritized DQN\", max_reward=max_reward)\n",
    "      \n",
    "   print(\"Done!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
