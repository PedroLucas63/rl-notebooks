{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92875a06",
      "metadata": {
        "id": "92875a06"
      },
      "source": [
        "# Preparação do Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "344a98f4",
      "metadata": {
        "id": "344a98f4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
        "import ale_py\n",
        "\n",
        "gym.register_envs(ale_py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d9d760ac",
      "metadata": {
        "id": "d9d760ac"
      },
      "outputs": [],
      "source": [
        "def moving_average(rewards, window_size=10):\n",
        "    n = len(rewards)\n",
        "    grouped_means = [np.mean(rewards[i:i+window_size]) for i in range(0, n, window_size)]\n",
        "    grouped_indices = [i for i in range(0, n, window_size)]\n",
        "    return grouped_indices, grouped_means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "20bfe5a1",
      "metadata": {
        "id": "20bfe5a1"
      },
      "outputs": [],
      "source": [
        "def plot_rewards(train_rewards, test_rewards=None, title=None, max_reward=None, window_size=10):\n",
        "    if test_rewards is not None:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    else:\n",
        "        fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
        "        axes = [axes]\n",
        "\n",
        "    # TRAINING\n",
        "    axes[0].plot(train_rewards, label='Reward (puro)', color='skyblue', alpha=0.6)\n",
        "    train_x, train_avg = moving_average(train_rewards, window_size)\n",
        "    axes[0].plot(train_x, train_avg, label=f'Média a cada {window_size}', color='orange')\n",
        "    axes[0].set_title(\"Training\")\n",
        "    axes[0].set_xlabel(\"Episodes\")\n",
        "    axes[0].set_ylabel(\"Reward\")\n",
        "\n",
        "    if max_reward is not None:\n",
        "        axes[0].axhline(max_reward, color='red', linestyle='--', label='Max Reward')\n",
        "\n",
        "    axes[0].legend()\n",
        "\n",
        "    # TESTING\n",
        "    if test_rewards is not None:\n",
        "        axes[1].plot(test_rewards, label='Reward (puro)', color='skyblue', alpha=0.6)\n",
        "        test_x, test_avg = moving_average(test_rewards, window_size)\n",
        "        axes[1].plot(test_x, test_avg, label=f'Média a cada {window_size}', color='orange')\n",
        "        axes[1].set_title(\"Testing\")\n",
        "        axes[1].set_xlabel(\"Episodes\")\n",
        "        axes[1].set_ylabel(\"Reward\")\n",
        "\n",
        "        if max_reward is not None:\n",
        "            axes[1].axhline(max_reward, color='red', linestyle='--', label='Max Reward')\n",
        "\n",
        "        axes[1].legend()\n",
        "\n",
        "    if title:\n",
        "        fig.suptitle(title)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20529315",
      "metadata": {
        "id": "20529315"
      },
      "source": [
        "# Prioritized Experience Replay (PER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3edf80b",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SumTree:\n",
        "   def __init__(self, capacity: int):\n",
        "      self.capacity = capacity\n",
        "      # A árvore tem (2 * capacity - 1) nós no total.\n",
        "      self.tree = np.zeros(2 * capacity - 1)\n",
        "      self.data_pointer = 0\n",
        "\n",
        "   def add(self, priority: float, data_index: int):\n",
        "      \"\"\" Adiciona uma nova prioridade na árvore. \"\"\"\n",
        "      tree_index = self.data_pointer + self.capacity - 1\n",
        "      \n",
        "      self.update(tree_index, priority)\n",
        "      \n",
        "      self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
        "\n",
        "   def update(self, tree_index: int, priority: float):\n",
        "      \"\"\" Atualiza a prioridade de um nó e propaga a mudança para cima. \"\"\"\n",
        "      change = priority - self.tree[tree_index]\n",
        "      self.tree[tree_index] = priority\n",
        "      \n",
        "      # Propaga a mudança para a raiz\n",
        "      while tree_index != 0:\n",
        "            tree_index = (tree_index - 1) // 2\n",
        "            self.tree[tree_index] += change\n",
        "\n",
        "   def get_leaf(self, value: float) -> (int, float, int):\n",
        "      \"\"\" Encontra a amostra (índice da árvore, prioridade, índice do dado) para um dado valor. \"\"\"\n",
        "      parent_index = 0\n",
        "      while True:\n",
        "            left_child_index = 2 * parent_index + 1\n",
        "            right_child_index = left_child_index + 1\n",
        "            \n",
        "            # Chegou em uma folha\n",
        "            if left_child_index >= len(self.tree):\n",
        "               leaf_index = parent_index\n",
        "               break\n",
        "            else:\n",
        "               if value <= self.tree[left_child_index]:\n",
        "                  parent_index = left_child_index\n",
        "               else:\n",
        "                  value -= self.tree[left_child_index]\n",
        "                  parent_index = right_child_index\n",
        "      \n",
        "      data_index = leaf_index - self.capacity + 1\n",
        "      return leaf_index, self.tree[leaf_index], data_index\n",
        "\n",
        "   @property\n",
        "   def total_priority(self) -> float:\n",
        "      \"\"\" Retorna a prioridade total (valor na raiz). \"\"\"\n",
        "      return self.tree[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac7aee1",
      "metadata": {
        "id": "bac7aee1"
      },
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "   def __init__(self, capacity: int, alpha: float = 0.6):\n",
        "      self._alpha = alpha\n",
        "      self._capacity = capacity\n",
        "      self._size = 0\n",
        "      self._index = 0\n",
        "      \n",
        "      # Estruturas de dados\n",
        "      self._memory = np.empty(capacity, dtype=object)\n",
        "      self._tree = SumTree(capacity)\n",
        "      \n",
        "      # Constantes\n",
        "      self.PER_e = 0.01\n",
        "      self.max_priority = 1.0\n",
        "\n",
        "   def __len__(self):\n",
        "      return self._size\n",
        "\n",
        "   def store(self, state, action, reward, next_state, done):\n",
        "      \"\"\" Armazena uma nova transição e dá a ela a prioridade máxima. \"\"\"\n",
        "      experience = (state, action, reward, next_state, done)\n",
        "      self._memory[self._index] = experience\n",
        "      \n",
        "      # Adiciona a prioridade na SumTree\n",
        "      self._tree.add(self.max_priority, self._index)\n",
        "      \n",
        "      # Atualiza os ponteiros\n",
        "      self._index = (self._index + 1) % self._capacity\n",
        "      self._size = min(self._size + 1, self._capacity)\n",
        "\n",
        "   def sample(self, batch_size: int, beta: float = 0.4):\n",
        "      \"\"\" Amostra um batch de transições usando a SumTree. \"\"\"\n",
        "      assert self._size >= batch_size, \"Buffer tem menos amostras que o batch_size\"\n",
        "\n",
        "      batch_indices = np.empty(batch_size, dtype=np.int32)\n",
        "      tree_indices = np.empty(batch_size, dtype=np.int32)\n",
        "      priorities = np.empty(batch_size, dtype=np.float32)\n",
        "\n",
        "      segment = self._tree.total_priority / batch_size\n",
        "      \n",
        "      for i in range(batch_size):\n",
        "         a = segment * i\n",
        "         b = segment * (i + 1)\n",
        "         value = np.random.uniform(a, b)\n",
        "         tree_idx, priority, data_idx = self._tree.get_leaf(value)\n",
        "         \n",
        "         priorities[i] = priority\n",
        "         batch_indices[i] = data_idx\n",
        "         tree_indices[i] = tree_idx\n",
        "\n",
        "      # Calcula os pesos de Importance Sampling (IS)\n",
        "      sampling_probabilities = priorities / self._tree.total_priority\n",
        "      weights = np.power(self._size * sampling_probabilities, -beta)\n",
        "      weights /= weights.max() # Normaliza para estabilidade\n",
        "\n",
        "      # Extrai os dados das transições\n",
        "      states, actions, rewards, next_states, dones = zip(*self._memory[batch_indices])\n",
        "      \n",
        "      return (\n",
        "         np.array(states, dtype=np.float32) / 255,\n",
        "         np.array(actions),\n",
        "         np.array(rewards, dtype=np.float32),\n",
        "         np.array(next_states, dtype=np.float32) / 255,\n",
        "         np.array(dones, dtype=np.int32),\n",
        "         weights.astype(np.float32),\n",
        "         tree_indices \n",
        "      )\n",
        "\n",
        "   def update_priorities(self, tree_indices: np.ndarray, errors: np.ndarray):\n",
        "      \"\"\" Atualiza as prioridades na SumTree após um passo de aprendizado. \"\"\"\n",
        "      priorities = (np.abs(errors) + self.PER_e) ** self._alpha\n",
        "      \n",
        "      for ti, p in zip(tree_indices, priorities):\n",
        "         self._tree.update(ti, p)\n",
        "      \n",
        "      self.max_priority = max(self.max_priority, np.max(priorities))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc4ceb8",
      "metadata": {
        "id": "dbc4ceb8"
      },
      "source": [
        "# Definição da Rede"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oqNJibSM3hSD",
      "metadata": {
        "id": "oqNJibSM3hSD"
      },
      "outputs": [],
      "source": [
        "class AtariDQN(nn.Module):\n",
        "  def __init__(self, action_shape):\n",
        "    super(AtariDQN, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(64 * 7 * 7, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, action_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view(x.size(0), -1)    \n",
        "    x = self.fc(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57048310",
      "metadata": {
        "id": "57048310"
      },
      "source": [
        "# Definição do Agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mtv8EpjF4DpD",
      "metadata": {
        "id": "Mtv8EpjF4DpD"
      },
      "outputs": [],
      "source": [
        "class PrioritizedAtariDQNAgent():\n",
        "   def __init__(self, env,\n",
        "                epsilon=1,\n",
        "                alpha=1e-3,\n",
        "                gamma=0.999,\n",
        "                epsilon_decay=0.998,\n",
        "                epsilon_min=0.01,\n",
        "                batch_size=64,\n",
        "                replay_buffer_size=100000,\n",
        "                beta=0.4,\n",
        "                beta_decay=1e-5):\n",
        "      self.epsilon = epsilon\n",
        "      self.alpha = alpha\n",
        "      self.gamma = gamma\n",
        "      self.epsilon_decay = epsilon_decay\n",
        "      self.epsilon_min = epsilon_min\n",
        "      self.batch_size = batch_size\n",
        "      self.beta = beta\n",
        "      self.beta_decay = beta_decay\n",
        "\n",
        "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.env = env\n",
        "\n",
        "      self.dqn = AtariDQN(env.action_space.n).to(self.device)\n",
        "      self.dqn_target = AtariDQN(env.action_space.n).to(self.device)\n",
        "\n",
        "      self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "      self.dqn_target.eval()\n",
        "\n",
        "      self.replay = PrioritizedReplayBuffer(replay_buffer_size, env.observation_space.shape)\n",
        "\n",
        "      self.optimizer = optim.Adam(self.dqn.parameters(), lr=alpha)\n",
        "      self.loss = nn.SmoothL1Loss(reduction='none')\n",
        "\n",
        "   def _update_dqn_target(self):\n",
        "      self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "\n",
        "   def act(self, state):\n",
        "      self.dqn.eval()\n",
        "\n",
        "      if np.random.rand() < self.epsilon:\n",
        "         return self.env.action_space.sample()\n",
        "\n",
        "      with torch.no_grad():\n",
        "         state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
        "         if state.ndim == 3:\n",
        "            state = state.unsqueeze(0)\n",
        "         action = self.dqn(state).argmax(dim=-1).item()\n",
        "\n",
        "      return action\n",
        "\n",
        "   def update(self):\n",
        "      self.dqn.train()\n",
        "\n",
        "      states, actions, rewards, next_states, dones, weights, indices = self.replay.sample(self.batch_size, self.beta)\n",
        "\n",
        "      states = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
        "      actions = torch.as_tensor(actions, dtype=torch.int64, device=self.device).unsqueeze(-1)\n",
        "      rewards = torch.as_tensor(rewards, dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
        "      next_states = torch.as_tensor(next_states, dtype=torch.float32, device=self.device)\n",
        "      dones = torch.as_tensor(dones, dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
        "      weights = torch.as_tensor(weights, dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
        "\n",
        "      q_eval = self.dqn(states).gather(-1, actions.long())\n",
        "\n",
        "      with torch.no_grad():\n",
        "         next_actions = self.dqn(next_states).argmax(dim=1, keepdim=True)\n",
        "         q_next = self.dqn_target(next_states).gather(1, next_actions)\n",
        "         q_target = rewards + self.gamma * q_next * (1 - dones)\n",
        "\n",
        "      losses = self.loss(q_eval, q_target)\n",
        "      loss = torch.mean(losses * weights)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      td_errors = (q_target - q_eval).detach().cpu().numpy().squeeze()\n",
        "      self.replay.update_priorities(indices, td_errors)\n",
        "\n",
        "      self.beta = min(1, self.beta + self.beta_decay)\n",
        "\n",
        "   def train(self, episodes, verbose=False):\n",
        "      progress = tqdm(range(1, episodes + 1), desc=\"Training\", disable=not verbose)\n",
        "      rewards = []\n",
        "\n",
        "      for episode in progress:\n",
        "         state, _ = self.env.reset()\n",
        "         episode_reward = 0\n",
        "         done = False\n",
        "\n",
        "         while not done:\n",
        "            action = self.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            self.replay.store(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if len(self.replay) > self.batch_size:\n",
        "               self.update()\n",
        "\n",
        "         if episode % 20 == 0:\n",
        "            self._update_dqn_target()\n",
        "\n",
        "         self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "\n",
        "         rewards.append(episode_reward)\n",
        "         progress.set_postfix({ \"Reward\": episode_reward, \"Epsilon\": self.epsilon })\n",
        "\n",
        "      return rewards\n",
        "\n",
        "   def evaluate(self, episodes, verbose=False):\n",
        "      progress = tqdm(range(1, episodes + 1), desc=\"Evaluating\", disable=not verbose)\n",
        "      rewards = []\n",
        "\n",
        "      epsilon_real = self.epsilon\n",
        "      self.epsilon = 0\n",
        "\n",
        "      self.dqn.eval()\n",
        "\n",
        "      for episode in progress:\n",
        "         state, _ = self.env.reset()\n",
        "         episode_reward = 0\n",
        "         done = False\n",
        "\n",
        "         while not done:\n",
        "            action = self.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "         rewards.append(episode_reward)\n",
        "         progress.set_postfix({ \"Reward\": episode_reward })\n",
        "\n",
        "      self.epsilon = epsilon_real\n",
        "\n",
        "      return rewards\n",
        "\n",
        "   def save(self, path):\n",
        "      torch.save(self.dqn.state_dict(), path)\n",
        "\n",
        "   def load(self, path):\n",
        "      self.dqn.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93b873c4",
      "metadata": {
        "id": "93b873c4"
      },
      "source": [
        "# Ambientes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "tIKkAkKz3JzB",
      "metadata": {
        "id": "tIKkAkKz3JzB"
      },
      "outputs": [],
      "source": [
        "envs = [\n",
        "    (\"ALE/Pong-v5\",           \"Pong\",           \"pong_per.pt\"         ),\n",
        "    (\"ALE/Breakout-v5\",       \"Breakout\",       \"breakout_per.pt\"     ),\n",
        "    (\"ALE/SpaceInvaders-v5\",  \"SpaceInvaders\",  \"spaceinvaders_per.pt\"),\n",
        "    (\"ALE/Qbert-v5\",          \"Q*Bert\",         \"qbert_per.pt\"        ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "lekLicR64PRC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "lekLicR64PRC",
        "outputId": "6d2a35f9-dbd7-4720-aa31-fcc7956eb0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Pong...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   1%|          | 10/1000 [05:53<9:43:50, 35.38s/it, Reward=-20, Epsilon=0.98]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m env = FrameStackObservation(env, \u001b[32m4\u001b[39m)\n\u001b[32m      8\u001b[39m agent = PrioritizedAtariDQNAgent(env, beta_decay=(\u001b[32m1\u001b[39m - \u001b[32m0.4\u001b[39m) / \u001b[32m1000\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_rewards = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m test_rewards = agent.evaluate(\u001b[32m100\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     13\u001b[39m agent.save(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcheckpoint_atari/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mPrioritizedAtariDQNAgent.train\u001b[39m\u001b[34m(self, episodes, verbose)\u001b[39m\n\u001b[32m     89\u001b[39m episode_reward = \u001b[32m0\u001b[39m\n\u001b[32m     90\u001b[39m done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     93\u001b[39m    action = \u001b[38;5;28mself\u001b[39m.act(state)\n\u001b[32m     94\u001b[39m    next_state, reward, terminated, truncated, _ = \u001b[38;5;28mself\u001b[39m.env.step(action)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "for env_name, env_title, path in envs:\n",
        "   print(f\"Training {env_title}...\")\n",
        "\n",
        "   env = gym.make(env_name, render_mode=\"rgb_array\", frameskip=1)\n",
        "   env = AtariPreprocessing(env, frame_skip=4, screen_size=84, grayscale_obs=True)\n",
        "   env = FrameStackObservation(env, 4)\n",
        "\n",
        "   agent = PrioritizedAtariDQNAgent(env, beta_decay=(1 - 0.4) / 1000)\n",
        "\n",
        "   train_rewards = agent.train(1000, verbose=True)\n",
        "   test_rewards = agent.evaluate(100, verbose=True)\n",
        "\n",
        "   agent.save(f'checkpoint_atari/{path}')\n",
        "\n",
        "   plot_rewards(train_rewards, test_rewards, f\"{env_title} - Prioritized DQN\")\n",
        "\n",
        "   print(\"Done!\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
